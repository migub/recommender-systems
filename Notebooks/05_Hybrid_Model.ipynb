{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/migub/recommender-systems/blob/main/Notebooks/05_Hybrid_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw_jLhGQBRsO"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJDKzDf_nzI6",
        "outputId": "3c22be65-e2aa-43d1-cd16-0d4c138f8aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.11/dist-packages (1.1.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## 1. Setup\n",
        "!pip install scikit-surprise lightgbm imbalanced-learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For model building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix)\n",
        "\n",
        "# LightGBM\n",
        "import lightgbm as lgb\n",
        "\n",
        "# For matrix factorization (Surprise)\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
        "from surprise.model_selection import GridSearchCV as surprise_GridSearch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Data Loading\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Recommender_Systems/train_preprocessed.csv\")"
      ],
      "metadata": {
        "id": "nlespVHsuxUA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FK5x5nbn9-f",
        "outputId": "79f6d17e-d92f-4237-f26b-4eb52a4dafdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "  X_train: (5699117, 15)\n",
            "  y_train: (5699117,)\n",
            "  X_val:   (1424780, 15)\n",
            "  y_val:   (1424780,)\n"
          ]
        }
      ],
      "source": [
        "## 3. Prepare Training Data\n",
        "df = df.drop(columns=['release_date', 'ts_listen'], errors='ignore')  # for example\n",
        "\n",
        "# Convert booleans to int if needed\n",
        "if df['is_listened'].dtype == bool:\n",
        "    df['is_listened'] = df['is_listened'].astype(int)\n",
        "\n",
        "# Let's separate the target\n",
        "target_col = 'is_listened'\n",
        "y = df[target_col]\n",
        "X = df.drop(columns=[target_col])\n",
        "\n",
        "# 2.2 Train-Validation Split for final ensemble\n",
        "# We'll hold out 20% for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"  X_train:\", X_train.shape)\n",
        "print(\"  y_train:\", y_train.shape)\n",
        "print(\"  X_val:  \", X_val.shape)\n",
        "print(\"  y_val:  \", y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FoW4MCEVqQV2"
      },
      "outputs": [],
      "source": [
        "## 4. MATRIX FACTORIZATION (using Surprise SVD)\n",
        "train_df = pd.DataFrame({\n",
        "    'user_id': X_train['user_id'],\n",
        "    'item_id': X_train['media_id'],\n",
        "    'rating': y_train  # 0 or 1\n",
        "})\n",
        "\n",
        "val_df = pd.DataFrame({\n",
        "    'user_id': X_val['user_id'],\n",
        "    'item_id': X_val['media_id'],\n",
        "    'rating': y_val\n",
        "})\n",
        "\n",
        "# Surprise requires a \"Reader\" that defines the rating_scale\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "\n",
        "# Build the full Surprise dataset from train_df\n",
        "train_data = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
        "trainset = train_data.build_full_trainset()\n",
        "\n",
        "# We'll also build a separate \"testset\" object from val_df for evaluating predictions\n",
        "# Surprise's testset format is list of (user, item, rating)\n",
        "valset = list(val_df[['user_id','item_id','rating']].itertuples(index=False, name=None))\n",
        "\n",
        "# 4.2 Train an SVD model\n",
        "svd_model = SVD(n_factors=50, random_state=42)  # tweak n_factors as needed\n",
        "svd_model.fit(trainset)\n",
        "\n",
        "# 4.3 Generate predictions on validation set\n",
        "# We'll store these predictions as a Pandas Series, aligned with X_val order\n",
        "val_preds_svd = []\n",
        "for row in valset:\n",
        "    user, item, true_rating = row\n",
        "    pred = svd_model.predict(user, item)\n",
        "    val_preds_svd.append(pred.est)\n",
        "\n",
        "# Convert list of predictions to a Series\n",
        "val_preds_svd = pd.Series(val_preds_svd, index=X_val.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ORhxQepTqe10"
      },
      "outputs": [],
      "source": [
        "## 5. LOGISTIC REGRESSION\n",
        "cat_cols = ['platform_name', 'platform_family', 'listen_type', 'user_gender', 'listen_hour_period', 'listen_weekpart']\n",
        "num_cols = ['context_type', 'media_duration', 'user_age', 'song_age']\n",
        "\n",
        "# Subset X_train, X_val\n",
        "X_train_lr = X_train[cat_cols + num_cols].copy()\n",
        "X_val_lr   = X_val[cat_cols + num_cols].copy()\n",
        "\n",
        "# Use OneHotEncoder from scikit-learn\n",
        "ohe = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
        "ohe.fit(X_train_lr[cat_cols])\n",
        "\n",
        "# Transform cat cols\n",
        "X_train_cat_encoded = ohe.transform(X_train_lr[cat_cols])\n",
        "X_val_cat_encoded   = ohe.transform(X_val_lr[cat_cols])\n",
        "\n",
        "# Combine with numeric columns\n",
        "X_train_num = X_train_lr[num_cols].fillna(0).to_numpy()\n",
        "X_val_num   = X_val_lr[num_cols].fillna(0).to_numpy()\n",
        "\n",
        "# We'll use scipy's sparse hstack to combine\n",
        "from scipy.sparse import hstack\n",
        "X_train_final_lr = hstack([X_train_cat_encoded, X_train_num])\n",
        "X_val_final_lr   = hstack([X_val_cat_encoded,   X_val_num])\n",
        "\n",
        "# 4.2 Fit Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train_final_lr, y_train)\n",
        "\n",
        "# 4.3 Predict probabilities on validation set\n",
        "val_preds_lr = lr_model.predict_proba(X_val_final_lr)[:, 1]\n",
        "val_preds_lr = pd.Series(val_preds_lr, index=X_val.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "swvNOlC4qioq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e16f47e-3eec-4ac1-8dd9-fbd04f831bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
            "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
          ]
        }
      ],
      "source": [
        "## 6. LIGHTGBM\n",
        "X_train_lgb = X_train_lr.copy()\n",
        "X_val_lgb   = X_val_lr.copy()\n",
        "\n",
        "# We'll reuse the same one-hot encoding logic from above\n",
        "X_train_cat_encoded_lgb = ohe.transform(X_train_lgb[cat_cols])\n",
        "X_val_cat_encoded_lgb   = ohe.transform(X_val_lgb[cat_cols])\n",
        "\n",
        "X_train_num_lgb = X_train_lgb[num_cols].fillna(0).to_numpy()\n",
        "X_val_num_lgb   = X_val_lgb[num_cols].fillna(0).to_numpy()\n",
        "\n",
        "X_train_final_lgb = hstack([X_train_cat_encoded_lgb, X_train_num_lgb])\n",
        "X_val_final_lgb   = hstack([X_val_cat_encoded_lgb,   X_val_num_lgb])\n",
        "\n",
        "# Convert sparse matrix to LightGBM Dataset\n",
        "# (You could train directly on the sparse matrix, but let's show the typical approach.)\n",
        "train_dataset_lgb = lgb.Dataset(X_train_final_lgb, label=y_train)\n",
        "\n",
        "# 6.2 Train LightGBM model (basic params)\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'verbosity': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "lgb_model = lgb.train(\n",
        "    params,\n",
        "    train_dataset_lgb,\n",
        "    num_boost_round=100,  # for example\n",
        ")\n",
        "\n",
        "# 6.3 Predictions on validation set\n",
        "val_preds_lgb = lgb_model.predict(X_val_final_lgb)\n",
        "val_preds_lgb = pd.Series(val_preds_lgb, index=X_val.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ss9BB9BUG-Vs"
      },
      "outputs": [],
      "source": [
        "## 7. ENSEMBLE: STACKING (Meta-Model)\n",
        "# 7.1 Create a \"meta-feature\" dataframe for the validation set\n",
        "meta_val_df = pd.DataFrame({\n",
        "    'svd': val_preds_svd,\n",
        "    'lr': val_preds_lr,\n",
        "    'lgb': val_preds_lgb\n",
        "}, index=X_val.index)\n",
        "\n",
        "# 7.2 Train a meta-model (Logistic Regression) on these 3 features\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(meta_val_df, y_val)\n",
        "final_val_preds = meta_model.predict_proba(meta_val_df)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hL3UNyabY_SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 8. EVALUATION\n",
        "def print_metrics(y_true, y_probs, threshold=0.5):\n",
        "    preds = (y_probs >= threshold).astype(int)\n",
        "    acc   = accuracy_score(y_true, preds)\n",
        "    prec  = precision_score(y_true, preds, zero_division=0)\n",
        "    rec   = recall_score(y_true, preds, zero_division=0)\n",
        "    f1    = f1_score(y_true, preds, zero_division=0)\n",
        "    auc   = roc_auc_score(y_true, y_probs)\n",
        "    print(f\"Threshold = {threshold}\")\n",
        "    print(f\" Accuracy:  {acc:.4f}\")\n",
        "    print(f\" Precision: {prec:.4f}\")\n",
        "    print(f\" Recall:    {rec:.4f}\")\n",
        "    print(f\" F1 Score:  {f1:.4f}\")\n",
        "    print(f\" ROC AUC:   {auc:.4f}\")\n",
        "    print()\n",
        "\n",
        "# 7.1 Evaluate each base model\n",
        "print(\"=== Base Models ===\")\n",
        "print(\"SVD Predictions:\")\n",
        "print_metrics(y_val, val_preds_svd)\n",
        "\n",
        "print(\"Logistic Regression:\")\n",
        "print_metrics(y_val, val_preds_lr)\n",
        "\n",
        "print(\"LightGBM:\")\n",
        "print_metrics(y_val, val_preds_lgb)\n",
        "\n",
        "# 7.2 Evaluate the stacked model\n",
        "print(\"=== Stacked Model ===\")\n",
        "print_metrics(y_val, final_val_preds)\n",
        "\n",
        "# 7.3 Confusion matrix for the stacked model at threshold=0.5\n",
        "from sklearn.metrics import confusion_matrix\n",
        "stacked_preds = (final_val_preds >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y_val, stacked_preds)\n",
        "print(\"Confusion Matrix (Stacked Model):\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h2omxacQ9MJ",
        "outputId": "ddc1316a-a6eb-4e43-efaa-8188cca6972d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Base Models ===\n",
            "SVD Predictions:\n",
            "Threshold = 0.5\n",
            " Accuracy:  0.7600\n",
            " Precision: 0.7861\n",
            " Recall:    0.8910\n",
            " F1 Score:  0.8353\n",
            " ROC AUC:   0.7987\n",
            "\n",
            "Logistic Regression:\n",
            "Threshold = 0.5\n",
            " Accuracy:  0.6898\n",
            " Precision: 0.6969\n",
            " Recall:    0.9655\n",
            " F1 Score:  0.8095\n",
            " ROC AUC:   0.6498\n",
            "\n",
            "LightGBM:\n",
            "Threshold = 0.5\n",
            " Accuracy:  0.6991\n",
            " Precision: 0.7144\n",
            " Recall:    0.9319\n",
            " F1 Score:  0.8088\n",
            " ROC AUC:   0.6755\n",
            "\n",
            "=== Stacked Model ===\n",
            "Threshold = 0.5\n",
            " Accuracy:  0.7628\n",
            " Precision: 0.7891\n",
            " Recall:    0.8906\n",
            " F1 Score:  0.8368\n",
            " ROC AUC:   0.8033\n",
            "\n",
            "Confusion Matrix (Stacked Model):\n",
            "[[220366 231562]\n",
            " [106433 866419]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOB+mN4cyjbFowEwZkAsqIK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}