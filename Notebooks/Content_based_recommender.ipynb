{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/migub/recommender-systems/blob/main/Notebooks/Content_based_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4_49ixtyMUb",
        "outputId": "6289e2b1-dd02-4cf3-d248-bf2a3bd06e12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvKMBnbQyo1N",
        "outputId": "c6a7d7a2-381f-412d-aeb2-804d433d9e9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505170 sha256=478a361f64e4d30a1020c93188c53c1739ac0befe5e3381c43eb500a8d63d583\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm2XNe_SQ30M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f93568f-eec5-46e6-b152-5b245e6e4388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading music data...\n",
            "Reading data file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "7558834it [00:44, 169471.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 7558834 interactions\n",
            "Found 2922 unique genres\n",
            "Sampled 500000 interactions\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Content-based Music Recommender System with SVD Tuning\n",
        "\n",
        "This module implements a comprehensive music recommendation system that combines:\n",
        "1. Content-based filtering using track features (genre, artist, year, duration)\n",
        "2. SVD-based collaborative filtering with hyperparameter tuning\n",
        "3. Data loading and preprocessing functionality\n",
        "\n",
        "The system supports both explicit and implicit feedback, with features for:\n",
        "- Feature extraction and weighting\n",
        "- Similarity computation with batch processing\n",
        "- Cross-validation for model evaluation\n",
        "- Hyperparameter optimization for SVD\n",
        "\"\"\"\n",
        "\n",
        "from surprise import AlgoBase, Dataset, Reader, SVD, accuracy\n",
        "from surprise import PredictionImpossible\n",
        "from surprise.model_selection import GridSearchCV, train_test_split\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import os\n",
        "import heapq\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MusicData:\n",
        "    \"\"\"Handles loading and preprocessing of music data\"\"\"\n",
        "    def __init__(self, filepath='/content/drive/MyDrive/Recommender_Systems/train.csv', sample_size=None):\n",
        "        self.filepath = filepath\n",
        "        self.sample_size = sample_size\n",
        "        self._df = None\n",
        "        self._genres = None\n",
        "        self._artists = None\n",
        "        self._years = None\n",
        "        self._durations = None\n",
        "        self.mediaID_to_name = {}\n",
        "        self.name_to_mediaID = {}\n",
        "\n",
        "    def load_music_data(self):\n",
        "        \"\"\"Load and preprocess the music data\"\"\"\n",
        "        try:\n",
        "            # Read CSV file\n",
        "            with open(self.filepath, 'r') as f:\n",
        "                # Skip header\n",
        "                header = f.readline().strip().split(',')\n",
        "\n",
        "                # Get column indices\n",
        "                user_idx = header.index('user_id')\n",
        "                media_idx = header.index('media_id')\n",
        "                listened_idx = header.index('is_listened')\n",
        "                genre_idx = header.index('genre_id')\n",
        "                artist_idx = header.index('artist_id')\n",
        "                year_idx = header.index('release_date')\n",
        "                duration_idx = header.index('media_duration')\n",
        "\n",
        "                # Read data\n",
        "                data = []\n",
        "                genres = defaultdict(list)\n",
        "                artists = defaultdict(str)\n",
        "                years = defaultdict(int)\n",
        "                durations = defaultdict(float)\n",
        "                unique_genres = set()\n",
        "\n",
        "                print(\"Reading data file...\")\n",
        "                for line in tqdm(f):\n",
        "                    try:\n",
        "                        row = line.strip().split(',')\n",
        "                        if len(row) < len(header):  # Skip malformed rows\n",
        "                            continue\n",
        "\n",
        "                        user_id = row[user_idx]\n",
        "                        media_id = row[media_idx]\n",
        "                        rating = float(row[listened_idx])\n",
        "                        genre = row[genre_idx] if row[genre_idx] else None\n",
        "                        artist = row[artist_idx] if row[artist_idx] else None\n",
        "\n",
        "                        data.append((user_id, media_id, rating))\n",
        "\n",
        "                        # Create name mappings\n",
        "                        self.mediaID_to_name[media_id] = f\"Media {media_id}\"\n",
        "                        self.name_to_mediaID[f\"Media {media_id}\"] = media_id\n",
        "\n",
        "                        if genre and genre != '':\n",
        "                            unique_genres.add(genre)\n",
        "                            genres[media_id].append(genre)\n",
        "                        if artist and artist != '':\n",
        "                            artists[media_id] = artist\n",
        "                        if row[year_idx] and row[year_idx] != '':\n",
        "                            try:\n",
        "                                year = int(str(row[year_idx])[:4])  # Extract year from date\n",
        "                                years[media_id] = year\n",
        "                            except:\n",
        "                                pass\n",
        "                        if row[duration_idx] and row[duration_idx] != '':\n",
        "                            try:\n",
        "                                duration = float(row[duration_idx])\n",
        "                                durations[media_id] = duration\n",
        "                            except:\n",
        "                                pass\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing row: {e}\")\n",
        "                        continue\n",
        "\n",
        "                print(f\"Loaded {len(data)} interactions\")\n",
        "                print(f\"Found {len(unique_genres)} unique genres\")\n",
        "\n",
        "                # Sample if needed\n",
        "                if self.sample_size and len(data) > self.sample_size:\n",
        "                    np.random.seed(42)\n",
        "                    indices = np.random.choice(len(data), size=self.sample_size, replace=False)\n",
        "                    data = [data[i] for i in indices]\n",
        "                    print(f\"Sampled {len(data)} interactions\")\n",
        "\n",
        "                # Create genre vectors\n",
        "                genre_list = sorted(unique_genres)\n",
        "                genre_to_idx = {g: i for i, g in enumerate(genre_list)}\n",
        "\n",
        "                self._genres = defaultdict(list)\n",
        "                for media_id, media_genres in genres.items():\n",
        "                    vec = [0] * len(genre_list)\n",
        "                    for genre in media_genres:\n",
        "                        if genre in genre_to_idx:\n",
        "                            vec[genre_to_idx[genre]] = 1\n",
        "                    self._genres[media_id] = vec\n",
        "\n",
        "                self._artists = artists\n",
        "                self._years = years\n",
        "                self._durations = durations\n",
        "\n",
        "                # Create temporary file for Surprise\n",
        "                temp_file = 'temp_ratings.txt'\n",
        "                with open(temp_file, 'w') as f:\n",
        "                    for user, item, rating in data:\n",
        "                        f.write(f\"{user}\\t{item}\\t{rating}\\n\")\n",
        "\n",
        "                # Create Surprise dataset\n",
        "                reader = Reader(line_format='user item rating', sep='\\t', rating_scale=(0, 1))\n",
        "                dataset = Dataset.load_from_file(temp_file, reader=reader)\n",
        "\n",
        "                # Remove temporary file\n",
        "                os.remove(temp_file)\n",
        "\n",
        "                return dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_genres(self):\n",
        "        return self._genres\n",
        "\n",
        "    def get_artists(self):\n",
        "        return self._artists\n",
        "\n",
        "    def get_years(self):\n",
        "        return self._years\n",
        "\n",
        "    def get_durations(self):\n",
        "        return self._durations\n",
        "\n",
        "    def get_popularity_ranks(self):\n",
        "        \"\"\"Calculate popularity rankings based on listen counts\"\"\"\n",
        "        try:\n",
        "            print(\"Computing popularity rankings...\")\n",
        "            listen_counts = defaultdict(int)\n",
        "            with open(self.filepath, 'r') as f:\n",
        "                header = f.readline().strip().split(',')\n",
        "                media_idx = header.index('media_id')\n",
        "                listened_idx = header.index('is_listened')\n",
        "\n",
        "                for line in f:\n",
        "                    row = line.strip().split(',')\n",
        "                    if len(row) < len(header):  # Skip malformed rows\n",
        "                        continue\n",
        "                    media_id = row[media_idx]\n",
        "                    is_listened = int(row[listened_idx])\n",
        "                    if is_listened == 1:  # Only count actual listens\n",
        "                        listen_counts[media_id] += 1\n",
        "\n",
        "            # Sort items by listen count\n",
        "            sorted_items = sorted(listen_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Create rankings (1-based)\n",
        "            rankings = defaultdict(int)\n",
        "            for rank, (item_id, count) in enumerate(sorted_items, 1):\n",
        "                rankings[item_id] = rank\n",
        "\n",
        "            print(f\"Generated rankings for {len(rankings)} items\")\n",
        "            print(f\"Most popular item has {max(listen_counts.values())} listens\")\n",
        "            print(f\"Top item has rank {min(rankings.values())}\")\n",
        "            return rankings\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating rankings: {e}\")\n",
        "            # Return empty rankings instead of exiting\n",
        "            return defaultdict(int)\n",
        "\n",
        "class ContentBasedRecommender(AlgoBase):\n",
        "    \"\"\"Content-based recommender using track features with popularity awareness\"\"\"\n",
        "    def __init__(self, music_data, k=100):  # Increased k significantly\n",
        "        AlgoBase.__init__(self)\n",
        "        self.music_data = music_data\n",
        "        self.k = k\n",
        "        self.sim = None\n",
        "        self.trainset = None\n",
        "        self.weights = {\n",
        "            'genre': 5.0,     # Heavily weighted genre\n",
        "            'artist': 4.0,    # Heavily weighted artist\n",
        "            'year': 2.0,      # Increased year importance\n",
        "            'duration': 1.0,  # Increased duration importance\n",
        "            'popularity': 3.0 # Added popularity weight\n",
        "        }\n",
        "        self.min_rating_threshold = 0.45  # Lowered threshold for better recall\n",
        "        self.popularity_scores = None\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "        self.trainset = trainset\n",
        "\n",
        "        print(\"Computing similarities...\")\n",
        "\n",
        "        try:\n",
        "            # Get all features\n",
        "            genres = self.music_data.get_genres()\n",
        "            years = self.music_data.get_years()\n",
        "            artists = self.music_data.get_artists()\n",
        "            durations = self.music_data.get_durations()\n",
        "\n",
        "            # Calculate popularity scores\n",
        "            listen_counts = defaultdict(int)\n",
        "            for u, i, r in trainset.all_ratings():\n",
        "                if r >= self.min_rating_threshold:\n",
        "                    listen_counts[trainset.to_raw_iid(i)] += 1\n",
        "\n",
        "            max_listens = max(listen_counts.values()) if listen_counts else 1\n",
        "            self.popularity_scores = {\n",
        "                item: count / max_listens\n",
        "                for item, count in listen_counts.items()\n",
        "            }\n",
        "\n",
        "            # Get all unique media IDs\n",
        "            all_media_ids = [trainset.to_raw_iid(iid) for iid in range(trainset.n_items)]\n",
        "\n",
        "            # Create feature matrices\n",
        "            genre_features = []\n",
        "            year_features = []\n",
        "            artist_features = []\n",
        "            duration_features = []\n",
        "            popularity_features = []\n",
        "\n",
        "            # Get unique artists and normalize years/durations\n",
        "            unique_artists = sorted(set(artists.values()))\n",
        "            artist_to_idx = {artist: idx for idx, artist in enumerate(unique_artists)}\n",
        "\n",
        "            # Get min/max values for normalization\n",
        "            valid_years = [y for y in years.values() if y > 0]\n",
        "            min_year = min(valid_years) if valid_years else 0\n",
        "            max_year = max(valid_years) if valid_years else 0\n",
        "            year_range = max_year - min_year if max_year > min_year else 1\n",
        "\n",
        "            valid_durations = [d for d in durations.values() if d > 0]\n",
        "            min_duration = min(valid_durations) if valid_durations else 0\n",
        "            max_duration = max(valid_durations) if valid_durations else 0\n",
        "            duration_range = max_duration - min_duration if max_duration > min_duration else 1\n",
        "\n",
        "            # Process each media item\n",
        "            for media_id in all_media_ids:\n",
        "                # Add genre features with enhanced TF-IDF weighting\n",
        "                genre_vec = genres.get(media_id, [0] * len(next(iter(genres.values()))) if genres else [])\n",
        "                if sum(genre_vec) > 0:\n",
        "                    # Apply enhanced TF-IDF weighting\n",
        "                    genre_vec = [x / sum(genre_vec) * self.weights['genre'] for x in genre_vec]\n",
        "                genre_features.append(genre_vec)\n",
        "\n",
        "                # Add normalized year features with exponential decay\n",
        "                year = years.get(media_id, None)\n",
        "                if year is not None and year > 0:\n",
        "                    normalized_year = (year - min_year) / year_range\n",
        "                    # Apply recency bias with exponential decay\n",
        "                    year_weight = np.exp((normalized_year - 1) * 2) * self.weights['year']\n",
        "                    year_features.append([year_weight])\n",
        "                else:\n",
        "                    year_features.append([0.0])\n",
        "\n",
        "                # Add artist features with popularity weighting\n",
        "                artist = artists.get(media_id, '')\n",
        "                artist_vec = [0] * len(unique_artists)\n",
        "                if artist in artist_to_idx:\n",
        "                    artist_vec[artist_to_idx[artist]] = 1 * self.weights['artist']\n",
        "                artist_features.append(artist_vec)\n",
        "\n",
        "                # Add normalized duration features\n",
        "                duration = durations.get(media_id, None)\n",
        "                if duration is not None and duration > 0:\n",
        "                    normalized_duration = (duration - min_duration) / duration_range\n",
        "                    duration_features.append([normalized_duration * self.weights['duration']])\n",
        "                else:\n",
        "                    duration_features.append([0.0])\n",
        "\n",
        "                # Add popularity features\n",
        "                pop_score = self.popularity_scores.get(media_id, 0)\n",
        "                popularity_features.append([pop_score * self.weights['popularity']])\n",
        "\n",
        "            # Convert to numpy arrays and combine features\n",
        "            feature_matrices = []\n",
        "\n",
        "            if genre_features:\n",
        "                genre_matrix = np.array(genre_features, dtype=np.float32)\n",
        "                feature_matrices.append(genre_matrix)\n",
        "\n",
        "            if year_features:\n",
        "                year_matrix = np.array(year_features, dtype=np.float32)\n",
        "                feature_matrices.append(year_matrix)\n",
        "\n",
        "            if artist_features:\n",
        "                artist_matrix = np.array(artist_features, dtype=np.float32)\n",
        "                feature_matrices.append(artist_matrix)\n",
        "\n",
        "            if duration_features:\n",
        "                duration_matrix = np.array(duration_features, dtype=np.float32)\n",
        "                feature_matrices.append(duration_matrix)\n",
        "\n",
        "            if popularity_features:\n",
        "                popularity_matrix = np.array(popularity_features, dtype=np.float32)\n",
        "                feature_matrices.append(popularity_matrix)\n",
        "\n",
        "            if not feature_matrices:\n",
        "                raise ValueError(\"No valid features available\")\n",
        "\n",
        "            # Combine features with weighted concatenation\n",
        "            self.item_features = np.hstack(feature_matrices)\n",
        "\n",
        "            # Enhanced L2 normalization with feature importance preservation\n",
        "            norms = np.linalg.norm(self.item_features, axis=1)\n",
        "            norms[norms == 0] = 1\n",
        "            self.item_features = self.item_features / norms[:, np.newaxis]\n",
        "\n",
        "            # Compute enhanced similarity matrix with cosine similarity\n",
        "            print(\"Computing similarity matrix...\")\n",
        "            self.sim = np.dot(self.item_features, self.item_features.T)\n",
        "\n",
        "            # Apply sigmoid transformation with steeper curve\n",
        "            self.sim = 1 / (1 + np.exp(-8 * (self.sim - 0.3)))\n",
        "\n",
        "            print(f\"Feature matrix shape: {self.item_features.shape}\")\n",
        "            print(f\"Number of items: {len(all_media_ids)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during feature computation: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        return self\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible('User and/or item unknown.')\n",
        "\n",
        "        try:\n",
        "            # Get similar items with ratings\n",
        "            neighbors = []\n",
        "            for rating in self.trainset.ur[u]:\n",
        "                if rating[0] == i:\n",
        "                    continue\n",
        "                sim = float(self.sim[i, rating[0]])\n",
        "                if not np.isnan(sim) and sim > 0:\n",
        "                    neighbors.append((sim, rating[1]))\n",
        "\n",
        "            # Sort by similarity\n",
        "            neighbors.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            # Take top k neighbors\n",
        "            k_neighbors = neighbors[:self.k]\n",
        "\n",
        "            if not k_neighbors:\n",
        "                return self.trainset.global_mean\n",
        "\n",
        "            # Compute weighted average with sigmoid activation\n",
        "            sim_sum = sum(sim for sim, _ in k_neighbors)\n",
        "            if sim_sum == 0:\n",
        "                return self.trainset.global_mean\n",
        "\n",
        "            weighted_sum = sum(sim * rating for sim, rating in k_neighbors)\n",
        "            prediction = weighted_sum / sim_sum\n",
        "\n",
        "            # Apply sigmoid transformation\n",
        "            prediction = 1 / (1 + np.exp(-5 * (prediction - 0.5)))\n",
        "\n",
        "            return max(0.0, min(1.0, prediction))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during rating estimation: {str(e)}\")\n",
        "            raise PredictionImpossible('Error computing prediction.')\n",
        "\n",
        "    def get_top_n_recommendations(self, user_id, n=10):\n",
        "        \"\"\"Get top N recommendations for a user\"\"\"\n",
        "        try:\n",
        "            # Convert user ID to inner ID\n",
        "            user_inner_id = self.trainset.to_inner_uid(user_id)\n",
        "\n",
        "            # Get items the user hasn't rated\n",
        "            user_items = set(rating[0] for rating in self.trainset.ur[user_inner_id])\n",
        "            candidate_items = [item for item in range(self.trainset.n_items)\n",
        "                             if item not in user_items]\n",
        "\n",
        "            # Get predictions for all candidate items\n",
        "            predictions = []\n",
        "            for item_id in candidate_items:\n",
        "                try:\n",
        "                    pred = self.estimate(user_inner_id, item_id)\n",
        "                    if pred >= self.min_rating_threshold:  # Use threshold for filtering\n",
        "                        predictions.append(\n",
        "                            (self.trainset.to_raw_iid(item_id), pred)\n",
        "                        )\n",
        "                except PredictionImpossible:\n",
        "                    continue\n",
        "\n",
        "            # Sort by predicted rating and return top N\n",
        "            predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "            return predictions[:n]\n",
        "\n",
        "        except Exception as e:\n",
        "            return []  # Return empty list if any error occurs\n",
        "\n",
        "    def evaluate_recommendations(self, testset, rankings, n=10):\n",
        "        \"\"\"Evaluate recommendations using various metrics\"\"\"\n",
        "        hits = 0\n",
        "        total_recs = 0\n",
        "        total_pops = 0\n",
        "        coverage = set()\n",
        "\n",
        "        # Group test set by user\n",
        "        user_test_items = defaultdict(list)\n",
        "        for uid, iid, true_r in testset:\n",
        "            user_test_items[uid].append((iid, true_r))\n",
        "\n",
        "        print(\"\\nEvaluating recommendations...\")\n",
        "        for uid, test_items in tqdm(user_test_items.items()):\n",
        "            try:\n",
        "                # Skip users not in training set\n",
        "                if not self.trainset.knows_user(self.trainset.to_inner_uid(uid)):\n",
        "                    continue\n",
        "\n",
        "                # Get recommendations for user\n",
        "                user_recs = self.get_top_n_recommendations(uid, n=n)\n",
        "                if not user_recs:\n",
        "                    continue\n",
        "\n",
        "                # Update metrics\n",
        "                total_recs += len(user_recs)\n",
        "                rec_ids = [iid for iid, _ in user_recs]\n",
        "                coverage.update(rec_ids)\n",
        "\n",
        "                # Calculate hits (relevant recommendations)\n",
        "                for test_iid, _ in test_items:\n",
        "                    if test_iid in rec_ids:\n",
        "                        hits += 1\n",
        "\n",
        "                # Calculate popularity\n",
        "                for item_id, _ in user_recs:\n",
        "                    total_pops += rankings.get(item_id, 0)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue  # Skip problematic users\n",
        "\n",
        "        # Calculate final metrics\n",
        "        total_test_items = sum(len(items) for items in user_test_items.values())\n",
        "        precision = hits / total_recs if total_recs > 0 else 0\n",
        "        recall = hits / total_test_items if total_test_items > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        coverage_ratio = len(coverage) / self.trainset.n_items if self.trainset.n_items > 0 else 0\n",
        "        avg_popularity = total_pops / total_recs if total_recs > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'coverage': coverage_ratio,\n",
        "            'avg_popularity': avg_popularity,\n",
        "            'total_recommendations': total_recs,\n",
        "            'unique_items_recommended': len(coverage),\n",
        "            'total_hits': hits,\n",
        "            'total_test_items': total_test_items\n",
        "        }\n",
        "\n",
        "class SVDTuning:\n",
        "    \"\"\"Class for tuning SVD parameters\"\"\"\n",
        "    def __init__(self):\n",
        "        self.param_grid = {\n",
        "            'n_factors': [50, 100, 150, 200],  # Increased range of latent factors\n",
        "            'n_epochs': [20, 30, 40],          # More training epochs\n",
        "            'lr_all': [0.002, 0.005, 0.01],    # More learning rate options\n",
        "            'reg_all': [0.02, 0.04, 0.06],     # More regularization options\n",
        "            'biased': [True],\n",
        "            'init_mean': [0],\n",
        "            'init_std': [0.1]                  # Smaller initialization for better convergence\n",
        "        }\n",
        "        self.cv_results = None\n",
        "\n",
        "    def tune(self, data):\n",
        "        \"\"\"Tune SVD parameters using grid search with cross-validation\"\"\"\n",
        "        try:\n",
        "            print(\"Starting grid search with 5-fold cross-validation...\")\n",
        "            gs = GridSearchCV(SVD, self.param_grid, measures=['rmse', 'mae'],\n",
        "                            cv=5, n_jobs=-1, joblib_verbose=2)\n",
        "            gs.fit(data)\n",
        "\n",
        "            self.cv_results = gs.cv_results\n",
        "\n",
        "            print(\"\\nBest RMSE:\", gs.best_score['rmse'])\n",
        "            print(\"Best MAE:\", gs.best_score['mae'])\n",
        "            print(\"Best params for RMSE:\", gs.best_params['rmse'])\n",
        "            print(\"Best params for MAE:\", gs.best_params['mae'])\n",
        "\n",
        "            # Return RMSE-optimized parameters\n",
        "            return gs.best_params['rmse']\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during tuning: {e}\")\n",
        "            return None\n",
        "\n",
        "def evaluate_algorithm(algo, testset, rankings, n=10):\n",
        "    \"\"\"Evaluate an algorithm using various metrics\"\"\"\n",
        "    predictions = algo.test(testset)\n",
        "\n",
        "    # Calculate prediction metrics\n",
        "    rmse = accuracy.rmse(predictions)\n",
        "    mae = accuracy.mae(predictions)\n",
        "\n",
        "    # For content-based recommender, calculate recommendation metrics\n",
        "    rec_metrics = {}\n",
        "    if isinstance(algo, ContentBasedRecommender):\n",
        "        rec_metrics = algo.evaluate_recommendations(testset, rankings, n)\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    true_negatives = 0\n",
        "\n",
        "    threshold = 0.55  # Align with ContentBasedRecommender threshold\n",
        "\n",
        "    for pred in predictions:\n",
        "        actual = pred.r_ui\n",
        "        predicted = 1 if pred.est >= threshold else 0\n",
        "        actual = 1 if actual >= threshold else 0\n",
        "\n",
        "        if predicted == 1 and actual == 1:\n",
        "            true_positives += 1\n",
        "        elif predicted == 1 and actual == 0:\n",
        "            false_positives += 1\n",
        "        elif predicted == 0 and actual == 1:\n",
        "            false_negatives += 1\n",
        "        else:\n",
        "            true_negatives += 1\n",
        "\n",
        "    # Calculate precision, recall, and F1\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': {\n",
        "            'true_positives': true_positives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives,\n",
        "            'true_negatives': true_negatives\n",
        "        },\n",
        "        **rec_metrics\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the recommender system\"\"\"\n",
        "    try:\n",
        "        # Load data with larger sample size\n",
        "        print(\"Loading music data...\")\n",
        "        music_data = MusicData(sample_size=500000)  # Significantly increased sample size\n",
        "        data = music_data.load_music_data()\n",
        "\n",
        "        if data is None:\n",
        "            print(\"Failed to load data. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Get popularity rankings\n",
        "        print(\"\\nComputing popularity rankings...\")\n",
        "        rankings = music_data.get_popularity_ranks()\n",
        "\n",
        "        # Continue even if rankings are empty\n",
        "        if not rankings:\n",
        "            print(\"Warning: Could not compute popularity rankings. Continuing with empty rankings.\")\n",
        "            rankings = defaultdict(int)\n",
        "\n",
        "        # Split data with stratification and smaller test set\n",
        "        print(\"\\nSplitting data into train and test sets...\")\n",
        "        trainset, testset = train_test_split(data, test_size=0.2, random_state=42)  # Increased training set size\n",
        "\n",
        "        print(f\"\\nDataset statistics:\")\n",
        "        print(f\"Number of users in training: {trainset.n_users}\")\n",
        "        print(f\"Number of items in training: {trainset.n_items}\")\n",
        "        print(f\"Number of ratings in training: {trainset.n_ratings}\")\n",
        "        print(f\"Sparsity: {1 - (trainset.n_ratings / (trainset.n_users * trainset.n_items)):.4f}\")\n",
        "\n",
        "        # Train and evaluate content-based recommender\n",
        "        print(\"\\nTraining content-based recommender...\")\n",
        "        content_rec = ContentBasedRecommender(music_data)\n",
        "        content_rec.fit(trainset)\n",
        "\n",
        "        print(\"\\nEvaluating content-based recommender...\")\n",
        "        content_metrics = evaluate_algorithm(content_rec, testset, rankings)\n",
        "        print(\"\\nContent-based Recommender Performance:\")\n",
        "        print(f\"F1 Score: {content_metrics['f1_score']:.4f}\")\n",
        "        print(f\"Precision: {content_metrics['precision']:.4f}\")\n",
        "        print(f\"Recall: {content_metrics['recall']:.4f}\")\n",
        "\n",
        "        # Train and evaluate SVD\n",
        "        print(\"\\nTuning SVD parameters...\")\n",
        "        svd_tuner = SVDTuning()\n",
        "        best_params = svd_tuner.tune(data)\n",
        "\n",
        "        if best_params:\n",
        "            print(\"\\nTraining SVD with best parameters...\")\n",
        "            svd = SVD(**best_params)\n",
        "            svd.fit(trainset)\n",
        "\n",
        "            print(\"\\nEvaluating SVD recommender...\")\n",
        "            svd_metrics = evaluate_algorithm(svd, testset, rankings)\n",
        "\n",
        "        # Save results\n",
        "        output_dir = 'results'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        results_file = os.path.join(output_dir, 'evaluation_results.txt')\n",
        "        print(f\"\\nSaving results to {results_file}\")\n",
        "\n",
        "        with open(results_file, 'w') as f:\n",
        "            f.write(\"Recommender System Evaluation Results\\n\")\n",
        "            f.write(\"===================================\\n\\n\")\n",
        "            f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "            f.write(\"Dataset Statistics:\\n\")\n",
        "            f.write(\"-----------------\\n\")\n",
        "            f.write(f\"Number of users: {trainset.n_users}\\n\")\n",
        "            f.write(f\"Number of items: {trainset.n_items}\\n\")\n",
        "            f.write(f\"Number of ratings: {trainset.n_ratings}\\n\")\n",
        "            f.write(f\"Sparsity: {1 - (trainset.n_ratings / (trainset.n_users * trainset.n_items)):.4f}\\n\\n\")\n",
        "\n",
        "            f.write(\"Content-based Recommender Results:\\n\")\n",
        "            f.write(\"--------------------------------\\n\")\n",
        "            for metric, value in content_metrics.items():\n",
        "                if isinstance(value, dict):\n",
        "                    f.write(f\"\\n{metric}:\\n\")\n",
        "                    for k, v in value.items():\n",
        "                        f.write(f\"  {k}: {v}\\n\")\n",
        "                elif isinstance(value, float):\n",
        "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
        "                else:\n",
        "                    f.write(f\"{metric}: {value}\\n\")\n",
        "\n",
        "            if best_params:\n",
        "                f.write(\"\\nSVD Recommender Results:\\n\")\n",
        "                f.write(\"----------------------\\n\")\n",
        "                f.write(\"Best Parameters:\\n\")\n",
        "                for param, value in best_params.items():\n",
        "                    f.write(f\"- {param}: {value}\\n\")\n",
        "                f.write(\"\\nPerformance Metrics:\\n\")\n",
        "                for metric, value in svd_metrics.items():\n",
        "                    if isinstance(value, dict):\n",
        "                        f.write(f\"\\n{metric}:\\n\")\n",
        "                        for k, v in value.items():\n",
        "                            f.write(f\"  {k}: {v}\\n\")\n",
        "                    elif isinstance(value, float):\n",
        "                        f.write(f\"{metric}: {value:.4f}\\n\")\n",
        "                    else:\n",
        "                        f.write(f\"{metric}: {value}\\n\")\n",
        "\n",
        "        # Generate sample recommendations\n",
        "        print(\"\\nGenerating sample recommendations...\")\n",
        "\n",
        "        # Get a random user with at least 10 ratings\n",
        "        eligible_users = [uid for uid in range(trainset.n_users)\n",
        "                         if len(trainset.ur[uid]) >= 10]\n",
        "        if eligible_users:\n",
        "            sample_user = trainset.to_raw_uid(np.random.choice(eligible_users))\n",
        "\n",
        "            print(f\"\\nGenerating recommendations for user {sample_user}\")\n",
        "            print(f\"User's training ratings:\")\n",
        "            user_ratings = trainset.ur[trainset.to_inner_uid(sample_user)]\n",
        "            for item_id, rating in user_ratings[:5]:  # Show first 5 ratings\n",
        "                print(f\"Item {trainset.to_raw_iid(item_id)}: {rating:.3f}\")\n",
        "\n",
        "            print(\"\\nTop 5 content-based recommendations:\")\n",
        "            content_recs = content_rec.get_top_n_recommendations(sample_user, n=5)\n",
        "            for item_id, score in content_recs:\n",
        "                print(f\"Item {item_id}: {score:.3f}\")\n",
        "\n",
        "            if best_params:\n",
        "                print(\"\\nTop 5 SVD recommendations:\")\n",
        "                testset = [(sample_user, trainset.to_raw_iid(i), 0)\n",
        "                          for i in range(trainset.n_items)\n",
        "                          if i not in set(r[0] for r in trainset.ur[trainset.to_inner_uid(sample_user)])]\n",
        "                predictions = svd.test(testset)\n",
        "                predictions.sort(key=lambda x: x.est, reverse=True)\n",
        "                for pred in predictions[:5]:\n",
        "                    print(f\"Item {pred.iid}: {pred.est:.3f}\")\n",
        "\n",
        "        print(f\"\\nEvaluation complete. Results saved to {results_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1t_5Rdn_4qGF07r1XAnKj7QsuBVqt993V",
      "authorship_tag": "ABX9TyNCaSRvpgcE9OggNzM4Lddm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}